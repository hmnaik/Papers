\documentclass{bmvc2k}

%% Enter your paper number here for the review copyu
\bmvcreviewcopy{123}

\title{Single View Correspondence Matching for Non-Coplanar Circles Using Euclidean Invariants}

% Enter the paper's authors in order
% \addauthor{Name}{email/homepage}{INSTITUTION_CODE}
\addauthor{Susan Student}{http://www.vision.inst.ac.uk/~ss}{1}
\addauthor{Petra Prof}{http://www.vision.inst.ac.uk/~pp}{1}
\addauthor{Colin Collaborator}{colin@collaborators.com}{2}

% Enter the institutions
% \addinstitution{Name\\Address}
\addinstitution{
 The Vision Institute\\
 University of Borsetshire\\
 Wimbleham, UK
}
\addinstitution{
 Collaborators, Inc.\\
 123 Park Avenue,\\
 New York, USA
}
\usepackage[]{algorithm2e}

% added by Yuji
%%%%%%%% FROM HERE
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{color}
\newcommand{\parmessage}[1]{\textcolor{cyan}{[Paragraph: #1]}}
\newcommand{\comments}[1]{\textcolor{magenta}{[Comments: #1]}}
\newcommand{\yuji}[1]{\textcolor{magenta}{[Yuji: #1]}}
\newcommand{\revise}[2]{\textcolor{red}{\sout{#1}} \textcolor{blue}{#2}}  %Before (magenta) and after (blue)
%\newcommand{\revise}[2]{\textcolor{green}{#1}}  %Modifications appear in green
%\newcommand{\revise}[2]{#1} % Just the modified result
%\newcommand{\revise}[2]{#2} % Before the modification
\usepackage{ulem}

\newcommand{\fref}[1]{Fig\bmvaOneDot~\ref{#1}}
\newcommand{\Fref}[1]{Figure~\ref{#1}}
\newcommand{\eref}[1]{Eq\bmvaOneDot~\ref{#1}}
\newcommand{\Eref}[1]{Equation~\ref{#1}}
\newcommand{\sref}[1]{Sec\bmvaOneDot~\ref{#1}}
\newcommand{\Sref}[1]{Section~\ref{#1}}
%%%%%%%% UNTIL HERE

% % % % % % Changed From Hemal
\newcommand{\hemal}[2]{\textcolor{red}{\sout{#1}} \textcolor{green}{#2}}
% % % % % End Changes Hemal 



\runninghead{Student, Prof, Collaborator}{BMVC Author Guidelines}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

%------------------------------------------------------------------------- 
% Document starts here
\begin{document}

\maketitle

\begin{abstract}
In this work we introduce a method to determine 2D-3D correspondences for non-coplanar circles using a single image, given that the 3D information is known. 
%the orientation of circle plane is known. 
The core idea of our method is to compute 3D information from 2D features, thereby transforming a 2D-3D problem to a 3D-3D problem. 
Earlier researchers suggested that a pair of non-coplanar circles preserves Euclidean invariants under perspective projection.
These invariants can be extracted from their image projections, but with a two fold ambiguity. 
%In this paper, we propose \textit{Conic pair descriptor} based on the Euclidean invariants.
%The proposed descriptor computes unique Euclidean invariants from a known 3D model and Euclidean invariants with two fold ambiguity from its image projections.
In this paper, we propose \textit{Conic pair descriptor}, which computes unique Euclidean invariants from known circular features on the 3D model and invariants with two fold ambiguity from its image projections.
%The proposed matching approach follows three steps to obtain correspondences between the circular features against the ambiguity.
In this paper, we include a detailed account of factors affecting the computation of invariants from conic projections. 
%\yuji{Our method can be used for any 3D object having identical circular features on different planes.}{}
We conduct experiments on real and synthetic models, in order to evaluate the proposed method. 
The experiment with synthetic images focuses on showing the impact of the size and plane orientation of the circles on the success of descriptor matching. 
3D models with artificial circular markers are prepared and 3D data is measured by using a photogrammetric measurement system. 
The results of the correspondence matching algorithm are compared against the ground truth, which is also generated from the measurement system. 
We also show that our method is robust against false positives and capable of supporting real-time applications.   

\end{abstract}

%------------------------------------------------------------------------- 
\section{Introduction}
\label{sec:intro}
Correspondence matching is one of the key problems in computer vision. 
Various vision problems such as pose estimation, object detection or model reconstruction rely on correspondence matching. 
Correspondence matching is identifying same set of features, either between two or more images (2D-2D matching) or between a model and its image (2D-3D matching).
Perspective projection differs the appearance of an object in the images, therefore correspondence matching needs feature descriptors that are immune to the projection.
Such feature descriptors consist of invariant quantities which can be computed from certain image features and matched directly with those obtained from the object features in case of the 2D-3D matching.
This paper focuses on the 2D-3D matching problem.

\par 
There are mainly two types of feature descriptors.
%One does not depend on existence of a specific features or shapes in the scene. 
%Methods like SIFT descriptor~\cite{Lowe1999} compute invariant quantities defined by local texture in the image.
One computes invariant quantities defined by local texture in an image such as SIFT descriptor~\cite{Lowe1999}.
%Recent trend computes some invariant quantities defined by local texture in image such as SIFT descriptor~\cite{Lowe1999}.
%Contrast to the above approaches, this type of approaches does not assume any primitive features in target scene.
%Alternatively, 
This type of feature descriptors is supposed to be used for textured scenes and textured objects so that we can use the descriptor for mobile applications, in which assumption on object shape is disfavored.
Such methods focus on improving their invariance as well as efficient memory usage so that the matching can run even on powerless processors in mobile phones~\cite{Ke2004,Calonder2010}.
One of the disadvantages is that their invariances are up to 2D transformation such as affine and rotation transformation around camera coordinate system.
To handle view change caused by perspective transformation, we must explicitly learn how invariant quantities are affected by perspective transformation~\cite{Kurz2012,Lepetit2006}. 

\par 
%Perspectivity is a Euclidean motion in space composed with a perspective projection. 
The other type of feature descriptors assume primitive features such as points, lines or conics to be present in the scene or on the target object. 
Advantage of using such specific features is that they are easier to detect from images.
%Various invariant quantities can be computed from planar and non-coplanar features \cite{forsyth_91}. 
%%The type of approaches are based on theory of invariants, which has been well-studied in the 90s. \cite{forsyth_91,gros_projective_1992}
%Forsyth \etal \cite{forsyth_91} and Gros and Quan \cite{gros_projective_1992} covered a detailed study on invariant quantities and their stability under perspective projection. 
Detailed studies on invariant quantities of planar and non-coplanar features as well as their stability under perspective projection have been conducted~\cite{forsyth_91,gros_projective_1992}.
%There exist several works that covered a detailed study on invariants from planar projective group and their stability under projective transformation \cite{forsyth_91,gros_projective_1992}.
%Especially, plane projective descriptors, which compute invariant quantities against perspective transformation from planar object, have been investigated.
The descriptors generated by plane projective groups have been proposed in various applications for correspondence matching~\cite{Matsunaga2000,uchiyama_random_2011,ying_camera_2007}.
In Augmented Reality~\cite{uchiyama_random_2011,van_rhijn_optical_2004} and industrial tracking applications~\cite{AICON}, planar patterns with lines or circles are specifically designed to support invariant computation for correspondence matching.  
% invariant quantities unaffected by perspective projection and such invariants are easier to compute from image features ~\cite{hartley_multiple_2003}. 
%Matsunaga \etal made a special chessboard with different pattern sizes and use cross ratio, which is a projective invariant quantity, defined by intersection of the patterns~\cite{Matsunaga2000}. 
%In this paper we will focus on a specific class of features, that is \textit{circles}. 

\par 
%Circles are one of the most primitive features, 
In industrial scenario, circles are widely present on a model, tracking target, as natural features or circular markers are attached to the model for photogrammetric measurements \cite{luhmann_close_2006} as shown in Fig. \ref{fig:introProblem}.
The state of the art measurement system involves taking multiple images of the model with encoded markers temporarily added in the scene to solve the correspondence problem. 
The encoded markers are removed after the measurements are done.  
In such cases it is favourable to build a tracking approach without the support of coded markers. 
A single view correspondence matching problem with coplanar circles and ellipses has widely been studied~\cite{lepetit_monocular_2005,forsyth_91,Ferri_1993}.
% Coplanar conics was used to compute cross ratio and  invariant quantity~\cite{forsyth_91,Ferri_1993}.
However matching problem with non-coplanar circles has not been addressed.
Figure \ref{fig:introProblem} shows an example of the matching problem, where multiple identical circular features exist on different planes of a 3D model.
In this case, features existing on the model is an advantage, but coplanar invariants can not be used for correspondence matching.
In our work we focus on solving this problem of single view matching of multiple non-coplanar conics.
%Recent works compute perspective~\cite{Nakai2005}, affine~\cite{Nakai2006}, rotation invariants~\cite{Uchiyama2011a} on key-points distribution defined by a set of key-points' location.
%Quan used algebric invariants section for finding correspondence between two images and 3D reconstruction \cite{quan_conic_1996}.
\begin{figure}[tb]
\centering
%\begin{tabular}{cc}
\subfigure{\includegraphics[width = 0.40\hsize]{images/Problem2.pdf} }  % \label{fig:Problem}
%Caption a : Example of industrial objects with circular markers used in close range Photogrammetry 
\subfigure{\includegraphics[width =0.50\hsize]{images/Objects2.pdf} } % \label{fig:Model}
%\end{tabular}
%caption b :
\caption{A primitive example explaining the correspondence problem when circular features exist on different planes of the model. Image on the left shows the problem with a simple cube. The other two images show complex 3D models with circular markers, Such markers are widely used in Industry for 3D surface measurements. }

\label{fig:introProblem}
\end{figure}

\par 
In case of non-coplanar features on 3D objects, Euclidean invariants are preserved rather than projective invariants \cite{forsyth_91}.
Euclidean invariants are difficult to extract from images due to perspective mapping, however circles are a special case.   
%Circles have a special property to retain depth information under projective transformation.
A world circle always produces an elliptical curve on the image plane. 
The ellipse in the image can be backprojected and the plane of the circle can be defined in camera frame with a two fold ambiguity \cite{forsyth_91,safaee-rad_three-dimensional_1992}. 
Further, Forsyth \etal \cite{forsyth_91} showed that for a given pair of non-coplanar circles in 3D, the angle between the circle planes and the distance between their centre positions are invariant quantities.
These invariants can be recovered from image projections of conics with a two fold ambiguity. 
%up to three projective invariant can be computed from a non-coplanar pair of circles.
%They explain that angle between circle planes (angle between surface normals) and distance between centre of the circles are invariant quantities.
The concept was proposed in early 90s, however these invariants have remained unexplored.

\par 
We propose using the aforementioned Euclidean invariants to solve the 2D-3D correspondence problem when multiple non-coplanar circular features exist on a model.
In our approach we bring the problem from 2D to 3D by backprojection image conics, then compute Euclidean invariant quantities to solve the matching problem from a single image.
In this paper, We introduce the \textit{Conic pair descriptor}, which encapsulates the invariants computed from elliptical image features.
%to solve the conic ambiguity and provide accurate matching with 3D features. 
Our contribution is a new method to accurately identify image correspondences when multiple identical non-coplanar circular features exist in the scene. 
The method assumes that the camera intrinsics are known and 3D information (i.e. size, normals, centre position) of features is available. 
In industry based model tracking application, the 3D-CAD data is often available. 
%Our matching method is suitable for tracking any 3D objects having known circles on different planes. 
%In close range photogrammetry circular markers (Figure. \ref{fig:Model}) are widely used on 3D models for surface measurements \cite{luhmann_close_2006}. 
%These measurements include computation of surface normal and 3D position of each marker. 
%The state of the art involves taking multiple images of the model with encoded markers added in the scene to solve the correspondence problem. 
%Once 3D measurements are done our method can be extremely useful to support a tracking application without using the coded patterns. %Similarly various industrial parts having natural circles can be identified and tracked with this method. 
The evaluation consists of synthetic experiments to understand factors influencing invariant computation and matching. 
%The proposed method is first attempt to use these invariants, therefore we also carried out simulations to show stability of invariants against change of perspective.
Additionally, real 3D models with circular markers as shown in Fig. \ref{fig:introProblem} are used to evaluate the matching method.
The proposed method can find corresponding circular markers from a single image, with high precision.
We also show that our method is stable against false positives and that it is fast enough to support real-time tracking applications. 

\section{Conic Invariants : Theory and Computation}
\label{Sec:ConicInv}
This section explains the theory of conic invariants derived by Forsyth \etal~\cite{forsyth_91}.
In this section we will describe Euclidean invariants preserved under perspective transformation defined by a pair of non-coplanar circles.

\par 
%\subsection{Conic invariants in 3D space}\label{ssec:conicInv3D}
A rigid 3D model can be assumed as a set of rigidly coupled planes.
Suppose there exists a set of circles on different planes of the model and the normal vector $N_i \in \mathbb{R}^3$ and the centre position $M_i \in \mathbb{R}^3$ is known in object frame.
%\revise{Forsyth \etal \cite{forsyth_91} explained that in case of three dimensional objects invariant descriptors consists of Euclidean invariants rather than projective invariants. 
Forsyth \etal pointed out that between each pair of non-coplanar circles the following Euclidean invariant quantities are preserved under perspective transformation: angle between the conic planes $ \theta $ and the distance between their centre positions $ d $.
The angle between the planes is equivalent to the angle between the surface normals, thus is obtained as
\begin{align}
\theta_{i,j} = \angle(N_i,N_j),~~~i \neq j,
\label{eq:invTheta3d}
\end{align}
where $i$ and $j$ represent the index of the conics.
The distance between the conic centres $d$ is computed as
\begin{align}
d_{i,j} = {\rm dist}(M_i,M_j),~~~i \neq j.
\label{eq:invD3d}
\end{align} 
\par
%\subsection{Conic invariants from a single image}\label{ssec:conicInv2D}
We can recover both invariants, the angle $\theta$ and the distance $d$, from image projections if the size of the circles is known.
This is possible because image projection of a circle is always elliptical. 
This property can be exploited further by backprojecting the ellipse in camera space~($\mathbb{R}^3 $) to obtain plane orientation of the circle, using ellipse parameters $C_i$ and centre position $m_i$ of each image conic.
A method to backproject ellipses is described in~\cite{forsyth_91}, we can compute the normal vector ${Nc_i}^1,{Nc_i}^2 \in \mathbb{R}^3$ and the centre position ${Mc_i}^1, {Mc_i}^2 \in \mathbb{R}^3$ of $i$-th ellipse up to a two fold ambiguity as
\begin{align} \label{Eq:Backprojection}
\{ ( {Nc_i}^a,{Mc_i}^a ) \} = \text{EllipseBackprojection}(m_i,C_i),~~~ a = \{1,2\}, % \\
%\{ ( {Nc_j}^b,{Mc_j}^b ) \} = \text{EllipseBackprojection}(m_i,C_i),~~~ b = \{1,2\},
\end{align}
where the superscripts $a,b$ represent the ambiguous solutions.
The two fold ambiguity is referred as \textit{ Conic ambiguity} in this paper.
%$\{ ( {Nc_i}^a,{Mc_i}^a ) \},\{ ( {Nc_j}^a,{Mc_j}^a ) \}$ 
Once the normals and the centre positions are recovered for each conic, we can compute the angle $\theta_{i,j}$ and the distance $d_{i,j}$ between the two conics as Eqs.~\ref{eq:invTheta3d} and \ref{eq:invD3d}. 
\begin{align}\label{eq:inv2D}
\left. \begin{aligned}
d_{i,j}^{a,b} &= {\rm dist}({Mc_i}^a,{Mc_j}^b),\\
\theta_{i,j}^{a,b} &= \angle({Nc_i}^a,{Nc_j}^b),
\end{aligned}
\right. ~~~i \neq j, ~~~ a,b = \{ 1,2 \}.
\end{align}
It is evident that due to \textit{Conic ambiguity} we have four solutions for each invariant. 
Forsyth \etal explained that the solutions for $d_{i,j}^{a,b}$ are consistent, whereas only one solution of $\theta_{i,j}^{a,b}$ is correct ~\cite{forsyth_91}.
Following this, we regard that a pair of two non-coplanar conics in an image derives a unique $d_{i,j}$ and four $\theta_{i,j}^{a,b}$.


%\section{Conic Invariants : Theory and Computation}
%\label{subSec:ConicInv}
%In this part we will explain the theory and computational aspects behind generating Euclidean invariants from the image conics. 
%Let the camera projection centre be assumed as the vertex of a cone having the world circle is as its base.
%Now the image plane can be considered as a cutting plane $\pi$, which always creates an elliptical cross section of the cone.  
%A rotation transformation $R_c$ can be applied to the camera coordinate system such that the new image plane $\pi'$ intersects the cone as a perfect circle, with the z-axis passing through the centre of both the circles. 
%The normal of the plane ($\pi'$) is same as the base of the cone, therefore a normal $Nc_i$ to the plane $ \pi $ can be computed using $R_c$. 
%Similarly, using $ R_i $, distance between the plane $ \pi' $ and the base of the cone can be computed. 
%Additionally, 3D position of circle centre $Mc_i$ can be computed in the original camera coordinate frame.
%The normal $Nc_i$ and the point $Mc_i$ are sufficient to define the orientation of the circle in camera coordinate system.
%$ R_c $ is a combination of two rotations, one of the two has $\pm \phi$ rotation ambiguity, which in turn introduces a two fold ambiguity in the solution (See Eq.\ref{Eq:Backprojection}). We will refer the ambiguity as \textit{Conic ambiguity} and the method to obtain plane orientation as \textit{Ellipse backprojection}. The mathematical model for \textit{Ellipse backprojection} is extensively covered by both Forsyth \etal \cite{forsyth_91} and Diego \cite{lo_pez_de_ipin_a_trip:_2002}. 
%\begin{align} \label{Eq:Backprojection}
%\text{Ellipse Backprojection}(m_i,C_i) &\rightarrow \pi({Nc_i}^1,{Mc_i}^1),\pi'({Nc_i}^2,{Mc_i}^2) \\ \nonumber
%\text{Ellipse Backprojection}(m_j,C_j) &\rightarrow \pi({Nc_j}^1,{Mc_j}^1),\pi'({Nc_j}^2,{Mc_j}^2)
%\end{align}
%Forsyth \etal \cite{forsyth_91} explained that in case of three dimensional objects invariant descriptors consists of Euclidean invariants rather than projective invariants. 
%Various invariants can be computed from a pair of non-coplanar circles based on Ellipse backprojection. 
%We use following invariants for our method, 
%\begin{itemize}
%	\item \textbf{Angle between planes} ($\theta$) : It is same as the angle between the surface normals 
%	(i.e. $ \angle(Nc_i,Nc_j) $). $\theta$ can be recovered from the image conics without knowledge of the circle size , however due to \textit{Conic ambiguity} $ \theta $ will have 4 solutions out of which only one is correct. 
%	\item \textbf{Distance between circle centres} : This is the distance between $ Mc_i$ and $ Mc_j $. The length of the vector $d$ is invariant (object scale should be known) and consistent despite of the ambiguity. 
%\end{itemize}
%%It should be noted that ambiguity of Ellipse Backprojection produces 4 solutions for $\theta$, only one of which is correct. 
%%Other invariants can be computed from recovered normal and centre values. These invariants are unstable \cite{forsyth_91} as the error from both recovered components influences the computation.

\section{Method}
%\hemal{non-coplanar conic pair descriptor}
This section first introduces the proposed descriptor, called {\textit{Conic pair descriptor}}, in \sref{ssec:descriptor}.  Further, the matching method using \textit{Conic pair descriptors} is explained in detail in \sref{ssec:matching}.
We assume that the 2D data from the image and 3D data from the model is already available.
%focus on the proposed descriptor and the matching method in detail.
The 3D data includes the surface normal $N_i$, centre position $M_i$ and size $R_i$ (diameter) of each circle on the model. 
The 2D data includes circle centre $ m_i $ and conic matrix $ C_i $.
The surface normals ${Nc_i}^a$ and centre positions ${Mc_i}^a$ are recovered from the 2D data, where $a ={1,2}$ denotes the index of ambiguous solution. 
The 2D data is extracted from an input image by the following procedure:
ellipse detection given an input image~\cite{naimark_circular_2002};
conic parameters estimation from the detected ellipse~\cite{farin_geometry_1998}; and
the normal vector and the centre position recovery from the conic parameters~\cite{lo_pez_de_ipin_a_trip:_2002}.
%We have followed the approach presented by Naimark \cite{naimark_circular_2002} for ellipse detection and Farin \etal \cite{farin_geometry_1998} for computation of conic matrices from ellipse parameters. 
%and Fitzgibbon \cite{fitzgibbon_direct_1999} and fitting.

\subsection{Descriptor Generation}\label{ssec:descriptor}
This part mainly discusses the generation of \textit{Conic pair descriptor} from conic invariants. 
The invariants for 3D model are computed from available 3D data ($M_i,N_i$) using \eref{Eq:3Ddescriptors}. This descriptor does not contain any ambiguity.
The same set of invariants can be computed from corresponding image features using Eq. \ref{eq:inv2D}, where the recovered $ d $ component is unique and $ \theta $ component has four different solutions (Eq. \ref{Eq:2Ddescriptors}). 
This \textit{Conic ambiguity} can not be resolved without knowing the correspondence $ m_i \leftrightarrow M_i $.
In our approach, we pursue the idea that the existence of multiple features on the model can be used to find correspondences without solving the conic ambiguity. 
The conic invariants are used to generate \textit{Conic pair descriptors}, $ V $ and $ v $ represent the descriptors computed from the model data and the image data respectively.
The principle idea is to perform a descriptor matching between $ v $ and $ V $ to obtain $ m_i \leftrightarrow M_i $ correspondence.
The proposed \textit{Conic pair descriptor} structure,
%which also encapsulates the conic ambiguity, 
%\begin{align}
%\textit{Conic pair descriptor}_{model} &= V_{p} \langle d,\theta \rangle_{i,j} \label{Eq:3Ddescriptors} \\
%\textit{Conic pair descriptor}_{image} &= v_{q} \langle  d,\theta_{11},\theta_{12},\theta_{21},\theta_{22} \rangle_{i,j} \label{Eq:2Ddescriptors} 
%\end{align}
\begin{align}
\textit{Conic pair descriptor}_{model} &= V_{p} = \langle d_{i,j},\theta_{i,j} \rangle \label{Eq:3Ddescriptors} \\
\textit{Conic pair descriptor}_{image} &= v_{q} = 
			\langle  d_{i,j},\theta_{i,j}^{1,1},\theta_{i,j}^{1,2},\theta_{i,j}^{2,1},\theta_{i,j}^{2,2} \rangle \label{Eq:2Ddescriptors} 
\end{align}
where index $p$ represents world circles $i,j$ and index $q$ represents image conic pair $i,j$.
The reader should note that given a unique set of points and their corresponding normals in 3D camera space (e.g. depth images), PFH descriptor \cite{RusuDoctoralDissertation} computes similar invariants.
In our case, we use a single 2D image to extract ambiguous centre and normal information in 3D camera space.  
The \textit{Conic ambiguity} introduced by \textit{Ellipse backprojection} forces us to include ambiguity in the descriptor structure (Eq. \ref{Eq:2Ddescriptors}). 
%In this case, the concept can not be applied to the results of \textit{Ellipse backprojection} as the \textit{Conic ambiguity} restricts us from computing a unique set of invariants. 
Unlike popular methods, a \textit{Conic pair descriptor} represents two features at the same time. 
In order to uniquely represent a single conic using Euclidean invariants, at least three conic features are required. 
The addition of each conic feature adds 3 wrong solutions of $ \theta $ in the descriptor, moreover the matching must rely on detection of all the conics used for descriptor computation. 
Descriptors $ v_{\{1..q\}} $ are generated for each pair of detected $ n $ image conics, where $ q = \binom{n}{2} $. 
$ V_{\{1...p\}} $ are generated off-line as the 3D model data is already available.
In case of $ l $ circles on the model $ p~\leq~\binom{l}{2} $, as the circle pair not likely to appear in the same image can be rejected. 
After computing the \textit{Conic pair descriptors} the following 3 step matching approach is used to achieve $ m_i \leftrightarrow M_i $ correspondences. 

% -> Figure explaining the issue
\begin{figure}
\centering

%\subfigure[A problem showing four image points, and their correspondence with world points (1-A,2-B,C-3) and one false positive (D)]{\includegraphics[width=0.47\hsize]{images/matchingProblem.pdf}}  
%\subfigure[Image shows the simplified matching process for the problem suggested in (a), partial results of all 3 steps are shown. X denotes the correspondence in the voting matrix]{\includegraphics[width=0.45\hsize]{images/matchingProcess2.pdf}}
\includegraphics[width = 0.95\hsize]{images/Problem_3Step.png}
\caption{ The image shows an example matching problem with real correspondence relations, Image has one false positive detection F. A pictorial overview of the three steps used in the matching process are shown with respect to the given problem. %Pairwise Initial Matching results are obtained from Algorithm \ref{algo:PIM}, 
$ H_i $ represents index of matched descriptor pair.  
%Matching problem and the overview of the method to generate correspondence hypothesis
\label{fig:matchingAndProblem}}
\end{figure}

\subsection{Descriptor matching}
\label{ssec:matching}

\subsubsection{Step 1: Pairwise Initial Matching}
\label{subsubSec:PIM}
In this stage we compare the \textit{Conic pair descriptors} and find all possible $ v \leftrightarrow V $ relationships.
% ($ V \leftrightarrow v $).
Each descriptor represents a pair of conics, therefore this step is called pairwise matching.
The objective is to reduce the complexity of the problem by finding possible pair correspondences ($ v \leftrightarrow V $). 
Figure \ref{fig:matchingAndProblem} shows the possible outcome of step 1 with respect to the problem shown in the figure.
First, the unique $ d $ component between the descriptors is compared with $ T_d $ as threshold.
If a match is found, the unique $ \theta $ component of $ V $ is compared with all four values of $ \theta $ component in $ v $ with $ T_\theta $ as threshold.
% (Ref. Algorithm \ref{algo:PIM}) . 
The match is considered positive if any one of the four $ \theta $ values match, this may result into false matching as only one of the $ \theta $ is correct for a given pair. 
This explains that \textit{Conic ambiguity} may contribute into false matching. 
The results may show \textit{one to many} type of relation between the descriptors, observe $ H_4 $ and $ H_{10} $ in Fig. \ref{fig:matchingAndProblem}. 
False descriptor matching can also happen if false positives are present in the image. 
Similarly, symmetric orientation of features on the object may generate same invariants between multiple pairs, which can also contribute towards false descriptor matching. 
%$ T_{d} $ and $ T_\theta $ are the threshold values used to compare the respective components. 
%Even an accurate match does not solve individual correspondence in this stage. 

%\begin{algorithm}[tb] \label{algo:PIM}
%\SetKwData{threeDFv}{$ V_{p} $}\SetKwData{twoDFv}{$ v_{q} $}
%\SetKwData{ThresholdD}{$T_{d} $}\SetKwData{ThresholdA}{$T_\theta $}
%\SetKwFunction{compareDist}{compare$d$}\SetKwFunction{compareAngle}{compare$ \theta $}
%\SetKwFunction{Save}{SavePairResult}
%\emph{Goal : Find all possible \threeDFv similar to \twoDFv} \;
%%\emph{Initialisation : \ThresholdD = 10 , \ThresholdA = 5} \; 
%\ForAll{ $ V $, $ p \leftarrow 0 $ \KwTo $ n $}{	
%	\ForAll{ $ v $, $ q \leftarrow 0 $ \KwTo $ l $}{
%			% Project conic points 
%			\If (\tcp*[h] {compares $ d $ component} ){ \compareDist{~\threeDFv,\twoDFv} $ < $ \ThresholdD } {
%				\If (\tcp*[h] {compares $ \theta $ component} ){ \compareAngle{\threeDFv,\twoDFv} $ < $ \ThresholdA }{
%				\tcp{ All 4 solutions of $ \theta $ in $ v_q $ are checked } \;			
%				\Save{$ p,q $} \tcp{Save result} \;
%				}
%			}
%		}	
%	}
%\caption{Pairwise Initial Matching algorithm}
%\end{algorithm}

%------------------------------------------------------------------------- 
\subsubsection{Step 2: Pointwise Triplet Matching}
In this stage we simplify the problem further and obtain hypothesis on pointwise matching ($ m_i \leftrightarrow M_i $) by performing a verification on $ v \leftrightarrow V $ matching results. 
The objective is to compare the results of step 1 to identify correspondences and reject the false descriptor matches. 
We seek three $ v \leftrightarrow V $ results, such that they complement each other to form a unique three point $ m_i \leftrightarrow M_i $ hypothesis, as shown in Fig. \ref{fig:matchingAndProblem}. 
A simple two stage approach is used to generate a triplet matching hypothesis. 
\paragraph{Stage 1} Find any two results of Pairwise Initial Matching in which both the image and the model descriptors represent one and only one common conic. If such results exist then an initial triplet matching hypothesis can be proposed. 
As example we consider results $ H_1 , H_2 $ from Fig. \ref{fig:matchingAndProblem}.
\[
 V_{12} \leftrightarrow v_{AB},V_{13} \leftrightarrow v_{AC } \xrightarrow{\text{Triplet Hypothesis}} [1~2~3] \leftrightarrow [A~B~ C]
\]
In the example above we can see that conic 1 is common in $ V $ and image conic A is common in $ v $ among the two solutions. We can form an initial 3 point matching hypothesis with these results. 
\paragraph{Stage 2} Find a new descriptor matching pair which can verify the triplet matching hypothesis formed in the previous stage. In context of the example given in stage 1, the result $ H_3 $ (Fig. \ref{fig:matchingAndProblem}) can verify hypothesis of stage 1.  
%
%\begin{enumerate}
%\item[1] Find any two results of Pairwise Initial Matching in which both the image and the model descriptors represent one and only one common conic. If such results exist then an initial triplet matching hypothesis can be proposed. 
%\[
% V_{12} \leftrightarrow v_{AB},V_{13} \leftrightarrow v_{AC } \xrightarrow{\text{Triplet Hypothesis}} [1~2~3] \leftrightarrow [A~B~ C]
%\]
%In the example above we can see that world conic 1 and image conic A is common among the two solutions. We form a 3 point matching hypothesis with these results. 
%\item[2] Find a new descriptor matching pair which can verify the triplet matching hypothesis formed in the previous stage (e.g. $ V_{23} \leftrightarrow v_{BC}$).
%\end{enumerate}
The verified triplets are saved and others are rejected. 
The results may also contain false triplet matches, $ [F~ D ~C]\leftrightarrow [1~ 2~ 4]$ as depicted in Fig.\ref{fig:matchingAndProblem} is a wrong triplet. 
%If $ x $ number of results are generated in step 1, the number of pairs compared in this stage is $ \binom{x}{3} $. 

\subsubsection{Step 3: Correspondence Hypothesis }
\label{subSec:CHypo}
In this final step results of triplet matching are combined and a voting matrix is generated. 
For each $ m_i \leftrightarrow M_i $ correspondence verified in step 2, one vote is added to the corresponding matrix entry. 
As a result, all unique $ m_i \leftrightarrow M_i $ pair gaining maximum votes in the matrix are proposed as a correspondence hypothesis.
In Fig. \ref{fig:matchingAndProblem} the results are : A$ \leftrightarrow $1, B$ \leftrightarrow $2, C$ \leftrightarrow $3 , D$ \leftrightarrow $4, E$ \leftrightarrow $5. 
%In case of conflicting votes the respective $ m_i \leftrightarrow M_i $ relation is not considered. 
A minimum of three correspondences are required compute a pose of the object \cite{lepetit_monocular_2005}, if the camera intrinsics are known. 
We propose computing the pose by selecting the three most voted $ m_i \leftrightarrow M_i $ results from the matrix.  
Other correspondences having lower vote count can be verified by using the pose. 
In Fig. \ref{fig:matchingAndProblem}, A$ \leftrightarrow $1, B$ \leftrightarrow $2 and C$ \leftrightarrow $3 can be selected to compute the pose $ P $ , and D$ \leftrightarrow $4, E$ \leftrightarrow $5 relation can be verified using the pose $ P $.
If only 3 out of $ n $ conics are detected in the image, the pose can not be verified. 


%and results may not be reliable. 

\section{Evaluation}
In this section we will cover experiments carried out to comment on accuracy and robustness of the algorithm. 
%To the best of our knowledge no other application has attempted using the euclidean invariants generated by circles. 
The reader should note that the problem of achieving single image 2D-3D correspondences for non-coplanar circles has not been addressed earlier. 
Therefore, alternative methods for comparison are not available. 
We prepared two car models by attaching circular markers as shown in Fig. \ref{fig:introProblem}.
Model 1 has 20 markers of size $ R_i $ = 12 mm and Model 2 has 26 markers of size $ R_i $ = 5mm. 
These markers are widely accepted and used in the industrial domain for photogrammetric measurements.     
3D measurements of the markers are done with state of the art metrology system, and the ground truth is established by giving each model point a unique ID in the database. 
The markers are attached randomly and coplanar placement is avoided. 
A high resolution (2560x1920 pix) camera is used for the experiments and MATLAB~\cite{MATLAB} is used for synthetic experiments. 

\subsection{Preliminary experiment}
The quality of recovered plane from \textit{Ellipse backprojection} depends on distance from the camera $ r $ and the viewing angle $ \eta $~(angle between the image plane and the circle plane)\cite{werghi_pose_1996}. 
We performed simulations to understand the behaviour of \textit{Ellipse backprojection} with respect to both $ r $ and $ \eta $. 
The parameter $ r $ is varied from 500 to 2000 mm and $ \eta $ from 0-70$^\circ$ in step wise manner, 100 iterations are performed at each position. 
%Realistic values have been assumed for camera intrinsics and size of circle ($ R_i $ = 5,12 mm).
%Figure. \ref{fig:InvariantRecovery} shows results recorded with $ R_i = 12 $, and image noise as $ \sigma = 0.3 $. 
The results (Fig. \ref{fig:InvariantRecovery}) suggest that, at low viewing angles ($\eta $) 0-10$^\circ$ both normal and centre estimation errors are higher, at any given distance. 
%This can be explained by the fact that for 
When $ \eta \leq 10 $, the image projection of a circle is more circular than elliptical, therefore recovery of ellipse parameters may have errors. 
The estimation error grows at higher camera distance, however the error in normal recovery appears less sensitive to increase in camera distance than the error in centre recovery. 
The results obtained with $ R_i = 5$ mm show a similar pattern, although the magnitude of the error is higher as smaller image projections reduce the accuracy of computation of ellipse parameters.  
We also compared the ambiguous results of estimated centres $ {Mc_i}^1 $ and $ {Mc_i}^2 $. 
The maximum distance recorded between the two is $ \leq $  0.1 mm for $R_i$ = 12 mm, this suggests that ambiguity can be neglected for the recovered centre position. 
This behaviour also explains consistency of invariant $ d $ between two image conics. 
%The centre recovery is prone to higher errors with higher camera distances, however it produces a consistent solution for invariant $ d $. 
%After performing \textit{Ellipse backprojection} we have essentially transformed our problem from image space to three dimensional space. 
\begin{figure}[tb]
\centering
\begin{tabular}{cc}
\subfigure[Centre estimation error]{\includegraphics[width=0.35\hsize]{images/centerEstimationErrorR6_N3.pdf}}
\subfigure[Normal estimation error]{ \includegraphics[width=0.35\hsize]{images/NormalRecoveryError_R6_3.pdf} }
\end{tabular}
\caption{ Ellipse backprojection results,~Image noise = $ \sigma = 0.3 $, $ R_i = 12 mm $ \label{fig:InvariantRecovery} }
\end{figure}

%------------------------------------------------------------------------- 
\subsection{Correspondence Matching vs Threshold Settings}
The aim of this experiment is to understand the role of the threshold values $ T_{d}$ and $ T_\theta $ in $ m_i \leftrightarrow M_i $ matching results. 
In order to perform this experiment we took 75 images of Model 1 from different camera positions (Distance Range 500-2000 mm). 
%All the results (Table.~\ref{tab:Exp2}) are verified with ground truth data manually. 
As suggested in Sec. \ref{subSec:CHypo}, each image has at least four detected conics. 
The results are considered \textit{Not Converged} (NC) in case of less than three $ m_i \leftrightarrow M_i $ results.
%If hypothesis proposes less than 3 $ m_i \leftrightarrow M_i $ results we consider it as \textit{Not Converged}. %\textit{Correct} results are divided in two categories purely to provide detailed overview of algorithm performance. \textit{ Complete} refers to $ m_i \leftrightarrow M_i $ hypothesis being 100\% correct, \textit{Partial} results suggest that some of the matching results (excluding top 3 voted pairs) are not correct.  

\begin{table}[tb]
\centering
\caption{Correspondence matching with varying threshold settings } \label{table:ThresholdEffect}
\begin{tabular}{ | c | c | c | c | c | c| c |}
\hline
$ T_\theta $ & $ T_{d} $ & NC & Positive & FP & Precision & Recall \\ \hline
%$ T_\theta $ & $ T_{d} $ & {} & Complete & Partial & {}\\ \hline
5 & 5  & 13 & 62 & 0 & 100 & 82.6 \\
5 & 10 & 8 & 67 & 0 & 100 & 89.33\\
5 & 15 & 4 & 67  & 4 & 94.36 & 89.33\\ \hline
3 & 5  & 28 & 47  & 0 & 100 & 62.66 \\
3 & 10 & 21 & 54  & 0 & 100 & 72 \\
3 & 15 & 19 & 53  & 3 & 94.64 & 70.66 \\ \hline
\end{tabular} \\
\label{tab:Exp2}
\end{table}

\par 
%In Sec. \ref{Sec:ConicInv}, we learned that $ d $ recovery is weak and therefore a flexible threshold may be appropriate for matching. 
The results ( Table. \ref{tab:Exp2}) show that relaxed values of $ T_{d} $ impacts both precision and recall values in negative manner. 
On the other hand, very stringent thresholds lead to lower recall values. Therefore, a right balance of threshold can be selected to achieve higher precision and recall rates. Our preferred settings for experiments is $ T_{d} = 10 $ and $ T_\theta = 5 $. The selection may require change based on density of the features. 

\subsection{Descriptor Matching vs Marker Orientation}
This experiment is carried out synthetically to observe the effect of orientation of the circles on the individual descriptor matching.
Two circle planes are placed in different orientations and images are rendered from 1000 random camera positions for each orientation. Gaussian noise is added to the images to simulate the camera behaviour. 
The control parameters, the distance between circle centres $ d $ is varied from 10 - 150 (mm) and the angle between the planes $ \theta $ is varied from 10 - 80$ ^\circ $. 
The objective is to recover the descriptor components $ d $ and $ \theta $ from the images, compare them with the ground truth and measure the success rate. 
Realistic values are used for camera intrinsics, $ T_{d} $ = 10 and $ T_\theta $ = 5 are kept constant. 
Camera positions are chosen at random, x-y-z rotation range is $ \pm 70^\circ $, x-y translation range is $ \pm 500 $ mm, z-translation (Camera distance) range is 500 to 2000 mm. 

%\begin{table}[tb]
%\centering
%\caption{Descriptor Matching Analysis } \label{table:MatchingSuccess}
%\begin{tabular}{|c | c | c | c | c |}
%\hline
%$ R_i $ & $ \theta $ & $ d $ & Camera Distance (mm) & Min-Max Success($ \% $) \\ \hline
%5 & 10-40 & 10-150 & 500-2000 & 58-82 \\
% {}& 40-80 & 10-150 & 500-2000 & 35-65 \\ \hline
%% & 0-40 & 1200-2000 & 55-80 \\
%% & 40-90 & 1200-2000 & 35-55\\ 
%12 & 10-40 & 10-150 & 500-2000 & 64-86 \\
%{}& 40-80 & 10-150& 500-2000 & 40-69 \\ \hline
%%& 0-40 & 10-150& 2000-4000 & 40-60 \\
%%& 40-90 & 10-150 & 2000-4000 & 20-40 \\ \hline
% 20 & 10-40 & 10-150 & 500-2000 & 74-92 \\
%{} & 40-80 & 10-150 & 500-2000  & 48-77 \\ \hline 
%\end{tabular} \\
%\label{tab:Exp1}
%\end{table}

\begin{table}[tb]
\centering
\caption{Descriptor Matching Analysis } \label{table:MatchingSuccess}
\begin{tabular}{|c | c | c | c |}
\hline
$ R_i $ (mm) & $ \theta $  & $ d $ (mm) & Min-Max Success($ \% $) \\ \hline
5 & 10$ ^\circ $-40$ ^\circ $ & 10-150  & 58-82 \\
 {}& 40$ ^\circ $-80$ ^\circ $ & 10-150  & 35-65 \\ \hline
12 & 10$ ^\circ $-40$ ^\circ $ & 10-150  & 64-86 \\
{}& 40$ ^\circ $-80$ ^\circ $ & 10-150 & 40-69 \\ \hline
 20 & 10$ ^\circ $-40$ ^\circ $ & 10-150 & 74-92 \\
{} & 40$ ^\circ $-80$ ^\circ $ & 10-150 &  48-77 \\ \hline 
\end{tabular} \\
\label{tab:Exp1}
\end{table}

\par
The table \ref{table:MatchingSuccess} provides summary of key observations made during the experiment. 
The matching success shows inversely proportional relation with $ \theta $, independent of $ d $. 
We learn that descriptor matching is influenced more by the angle between the planes than the distance between the circle centres. 
It is also seen that success of matching can be improved by increasing the size of the circles. 
%even at higher values of $ \theta $. 
This experiments suggest that when feature placement is possible, it is advised to choose larger circles or surfaces with lower plane angles for improved matching results. 

\subsection{Robustness against false positives}
%This experiment aims to understand impact of false positives on the matching result. 
This experiment aims to show the robustness of the matching method against presence of false positives in the scene. 
In order to introduce false positives in the scene, Model 1 and Model 2 are placed in the same scene and images are captured from different positions. 
The matching method is provided 3D information of one model at a time, which in turn make the markers present on the other model act as false positives in the image. 
The same set of images are used for the two experiments and the results of the experiment are shown in Figure \ref{fig:Exp4}.
We learn that the false positives are completely rejected when matching is focused on Model 1, On the contrary in case of Model 2, precision and recall values suffer due to presence of false positives from Model 1. 
Markers on Model 1 have bigger size and therefore invariant recovery is strong. This can explain higher robustness of Model 1 against false positives. 

%\begin{table}[tb]
%\centering
%\caption{Robustness against False Positive } 
%\begin{tabular}{ | c | c | c | c | c | c | c |}
%\hline
%Model & Images & NC & Positive & FP & Precision & Recall \\ \hline
%Model 1 & 50 & 1  & 49 & 0  & 100 & 98 \\
%Model 2 & 50 & 4 & 31 & 15  & 67.39 & 62 \\ \hline
%\end{tabular} \\
%\label{tab:Exp4}
%\end{table}

\begin{figure}[tb]
\begin{minipage}{0.5\textwidth}
\centering
 \includegraphics[width=0.80\hsize]{images/Exp4.png}
 \caption{Robustness against False Positives}
  \label{fig:Exp4}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
  \includegraphics[width=0.80\hsize]{images/Exp5.png} 
 \caption{Time Analysis}
 \label{fig:Exp5}
\end{minipage}
%\subfigure[Robustness against False positives]{\includegraphics[width=0.40\hsize]{images/Exp4.png}}
%\subfigure[Time Analysis]{ \includegraphics[width=0.40\hsize]{images/Exp5.png} }
%\caption{ Ellipse backprojection results,Image noise = $ \sigma = 0.3 $, $ R_i = 12 mm $ \label{fig:InvariantRecovery} }
\end{figure}

%------------------------------------------------------------------------
\subsection{Time Analysis}
In this experiment we measure the time consumed by the matching method when introduced into a tracking application. 
Two cameras CAM 1~(2560 x 1920 pix) and CAM 2~(640x480) are used for tracking Model 1, the results presented are averaged over 100 frames. 
Tracking process is divided into 4 stages, the graph in Fig. \ref{fig:Exp5} depicts the time consumed by each stage.
The results show that the \textit{Matching} stage takes 3.09 ms with CAM 1 and 1.88 ms with CAM 2, which is $ \le 2\% $ of the total tracking time. 
In terms of performance, we achieve tracking speed of 3 FPS with CAM 1 and 8 FPS with CAM 2. 
The computer used has Intel Core i5 2.8~GHz processor with 8~GB RAM. 
Additionally, an experiment is performed with CAM 1 by adding 90-140 false positives in the scene. 
In this case, the \textit{Matching} stage is exhausted and consumes 6.277 seconds out of the total tracking time of 6.7599 seconds. 
CAM 2 has limited tracking range (i.e. $ < 1000~mm $) and low resolution, therefore accommodating 90-140 false positives in one scene within the tracking range is not possible. 
%The results show that our method takes $ \leq 1\% $ time from in the tracking pipeline. In terms of frame rates we achieve 2-3 FPS with CAM 1 and 7-8 FPS with CAM 2. Additionally, an exhaustive experiment with 90-140 false positives in the scene shows that the matching method consumes maximum time in the pipeline (0.7 FPS). Limited tracking range ($<$ 500 mm) of CAM 2 does not allow experiment with such large number of false positives. 
%\begin{table}[tb]
%\caption{Time Analysis}\label{tab:Exp5}
%\centering
%\begin{tabular}{|c | c | c | c |}
%\hline 
%Algorithm Stage & \multicolumn{2}{|c|}{CAM 1} & CAM 2\\ \hline
%{} & Model & Model + FP & Model \\ \hline 
%Image Undistortion & 39.53\% & 12 & 11.79 \\
%Marker Detection & 38.51 & 34 & 30.75 \\
%Correspondence Matching & 0.35 & 44.2 & 1.34 \\
%%Descriptor Generation & 0.10 & 0.33 \\
%%Descriptor Matching & 0.25 & 1.01 \\
%Pose Estimation & 21.61 & 9.8 & 56.12 \\ \hline
%\end{tabular}
%\end{table}

\section{Conclusion \& Future work }
%----------- Do I mention conic ambiguity? 
In this paper we have demonstrated a successful approach for solving 2D-3D correspondence matching problem for non-coplanar circular features from a single image. 
We propose a new \textit{Conic pair descriptor} which represents Euclidean invariants generated by a pair of non-coplanar circles.
Our method can successfully define correspondences when more than three circular features are present in the scene. 
The proposed method is the first to address the correspondence matching using these invariants since its introduction in the 90s.
Our contribution also includes providing detailed understanding of behaviour of invariants with respect to orientation and size of the circles. 
The major factors affecting matching are also discussed to optimize the method for best possible matching results based on the application (threshold settings, circle size, camera distance). 
The results of the experiments support our claim, that the method is fast, reliable and robust against false positives.
Our method can be used for object tracking or object identification in the industrial environment, where natural or artificial circular features exist predominantly on the models. 
However, the method is generic and can be used for any application dealing with non-coplanar circles. 
The 3D information of the features on the model and camera calibration are the only prerequisites for matching. 
The reader should also note that algorithm may not perform well with symmetric or coplanar arrangement of circular features. 

\par 
In context of future work, we would like to improve the method to be able to handle features of different sizes simultaneously.
Also a faster matching strategy is required to handle large number of feature points and false positives. We would like to use the same invariants to compute 2D-2D correspondence matching between two images in order to generate the 3D data which is a prerequisite now. We also consider using such a matching algorithm to support creating 3D markers for monocular Augmented Reality applications. This can be a cheap alternative to conventionally used 3D spherical markers.  


\bibliography{egbib,tracking}
\end{document}
